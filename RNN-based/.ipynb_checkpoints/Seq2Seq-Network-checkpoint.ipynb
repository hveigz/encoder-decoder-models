{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEQUENCE TO SEQUENCE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "europarl_en = open('data/europarl-v7.fr-en.en', encoding='utf-8').read().split('\\n')\n",
    "europarl_fr = open('data/europarl-v7.fr-en.fr', encoding='utf-8').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Resumption of the session', 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "\n",
      "['Reprise de la session', 'Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.']\n"
     ]
    }
   ],
   "source": [
    "print(europarl_en[:2])\n",
    "print()\n",
    "print(europarl_fr[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = spacy.load('en')\n",
    "fr = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_en(sentence):\n",
    "    return [tok.text for tok in en.tokenizer(sentence)]\n",
    "\n",
    "def tokenize_fr(sentence):\n",
    "    return [tok.text for tok in fr.tokenizer(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_TEXT = Field(tokenize=tokenize_en, pad_token=\"<pad>\")\n",
    "FR_TEXT = Field(tokenize=tokenize_fr, init_token = \"<sos>\", eos_token = \"<eos>\", pad_token=\"<pad>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {'English' : [line for line in europarl_en], 'French': [line for line in europarl_fr]}\n",
    "df = pd.DataFrame(raw_data, columns=[\"English\", \"French\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resumption of the session</td>\n",
       "      <td>Reprise de la session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.</td>\n",
       "      <td>Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.</td>\n",
       "      <td>Comme vous avez pu le constater, le grand \"bogue de l'an 2000\" ne s'est pas produit. En revanche, les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You have requested a debate on this subject in the course of the next few days, during this part-session.</td>\n",
       "      <td>Vous avez souhaité un débat à ce sujet dans les prochains jours, au cours de cette période de session.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.</td>\n",
       "      <td>En attendant, je souhaiterais, comme un certain nombre de collègues me l'ont demandé, que nous observions une minute de silence pour toutes les victimes, des tempêtes notamment, dans les différents pays de l'Union européenne qui ont été touchés.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                    English  \\\n",
       "0                                                                                                                                                                                                                 Resumption of the session   \n",
       "1                           I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.   \n",
       "2                                           Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.   \n",
       "3                                                                                                                                 You have requested a debate on this subject in the course of the next few days, during this part-session.   \n",
       "4  In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.   \n",
       "\n",
       "                                                                                                                                                                                                                                                  French  \n",
       "0                                                                                                                                                                                                                                  Reprise de la session  \n",
       "1                                                   Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.  \n",
       "2                             Comme vous avez pu le constater, le grand \"bogue de l'an 2000\" ne s'est pas produit. En revanche, les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.  \n",
       "3                                                                                                                                                 Vous avez souhaité un débat à ce sujet dans les prochains jours, au cours de cette période de session.  \n",
       "4  En attendant, je souhaiterais, comme un certain nombre de collègues me l'ont demandé, que nous observions une minute de silence pour toutes les victimes, des tempêtes notamment, dans les différents pays de l'Union européenne qui ont été touchés.  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove very long sentences and sentences where translations are \n",
    "# not of roughly equal length\n",
    "df['eng_len'] = df['English'].str.count(' ')\n",
    "df['fr_len'] = df['French'].str.count(' ')\n",
    "df = df.query('fr_len < 80 & eng_len < 80')\n",
    "df = df.query('fr_len < eng_len * 1.5 & fr_len * 1.5 > eng_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "      <th>eng_len</th>\n",
       "      <th>fr_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resumption of the session</td>\n",
       "      <td>Reprise de la session</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.</td>\n",
       "      <td>Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.</td>\n",
       "      <td>37</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.</td>\n",
       "      <td>Comme vous avez pu le constater, le grand \"bogue de l'an 2000\" ne s'est pas produit. En revanche, les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.</td>\n",
       "      <td>30</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You have requested a debate on this subject in the course of the next few days, during this part-session.</td>\n",
       "      <td>Vous avez souhaité un débat à ce sujet dans les prochains jours, au cours de cette période de session.</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.</td>\n",
       "      <td>En attendant, je souhaiterais, comme un certain nombre de collègues me l'ont demandé, que nous observions une minute de silence pour toutes les victimes, des tempêtes notamment, dans les différents pays de l'Union européenne qui ont été touchés.</td>\n",
       "      <td>39</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                    English  \\\n",
       "0                                                                                                                                                                                                                 Resumption of the session   \n",
       "1                           I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.   \n",
       "2                                           Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.   \n",
       "3                                                                                                                                 You have requested a debate on this subject in the course of the next few days, during this part-session.   \n",
       "4  In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.   \n",
       "\n",
       "                                                                                                                                                                                                                                                  French  \\\n",
       "0                                                                                                                                                                                                                                  Reprise de la session   \n",
       "1                                                   Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.   \n",
       "2                             Comme vous avez pu le constater, le grand \"bogue de l'an 2000\" ne s'est pas produit. En revanche, les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.   \n",
       "3                                                                                                                                                 Vous avez souhaité un débat à ce sujet dans les prochains jours, au cours de cette période de session.   \n",
       "4  En attendant, je souhaiterais, comme un certain nombre de collègues me l'ont demandé, que nous observions une minute de silence pour toutes les victimes, des tempêtes notamment, dans les différents pays de l'Union européenne qui ont été touchés.   \n",
       "\n",
       "   eng_len  fr_len  \n",
       "0        3       3  \n",
       "1       37      32  \n",
       "2       30      36  \n",
       "3       18      18  \n",
       "4       39      37  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and validation set \n",
    "train, val = train_test_split(df.drop([\"eng_len\",\"fr_len\"], axis=1).head(1000), test_size=0.1, random_state=12345)\n",
    "train.to_csv(\"data/train.csv\", index=False)\n",
    "val.to_csv(\"data/val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associate the text in the 'English' column with the EN_TEXT field, # and 'French' with FR_TEXT\n",
    "data_fields = [('English', EN_TEXT), ('French', FR_TEXT)]\n",
    "train,val = torchtext.data.TabularDataset.splits(path='data/', train='train.csv', \n",
    "                                       validation='val.csv', format='csv', \n",
    "                                       fields=data_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 2\n",
    "FR_TEXT.build_vocab(train, val, min_freq=min_freq)\n",
    "EN_TEXT.build_vocab(train, val, min_freq=min_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12345\n",
    "random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.English))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.French) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)\n",
    "\n",
    "class MyIterator(data.Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "            \n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                          self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "embed_size = 256\n",
    "lr = 0.0001\n",
    "grad_clip = 10\n",
    "epochs = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = MyIterator(train, batch_size=batch_size, device=0,\n",
    "                        repeat=False, sort_key= lambda x:\n",
    "                        (len(x.English), len(x.French)),\n",
    "                        batch_size_fn=batch_size_fn, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_iter = MyIterator(val, batch_size=batch_size, device=0,\n",
    "                        repeat=False, sort_key= lambda x:\n",
    "                        (len(x.English), len(x.French)),\n",
    "                        batch_size_fn=batch_size_fn, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, n_layers=1, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.embed = nn.Embedding(input_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, n_layers,\n",
    "                          dropout=dropout, bidirectional=True)\n",
    "\n",
    "    def forward(self, src, hidden=None):\n",
    "        embedded = self.embed(src)\n",
    "        outputs, hidden = self.gru(embedded, hidden)\n",
    "        # sum bidirectional outputs\n",
    "        outputs = (outputs[:, :, :self.hidden_size] +\n",
    "                   outputs[:, :, self.hidden_size:])        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, method):\n",
    "        super(Attention, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.rand(hidden_size))        \n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = encoder_outputs.size(0)\n",
    "        h = hidden.repeat(seq_len, 1, 1).transpose(0, 1) # [B*seq_len*H]\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)  # [B*seq_len*H]\n",
    "        attn_energies = self.score(h, encoder_outputs)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1) # [B*1*seq_len]\n",
    "                            \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.dot(encoder_output)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = hidden.dot(energy)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':                    \n",
    "            energy = F.tanh(self.attn(torch.cat([hidden, encoder_output], 2))) # [B*seq_len*2H]->[B*seq_len*H]\n",
    "            energy = energy.transpose(1, 2)  # [B*H*seq_len]\n",
    "            v = self.v.repeat(encoder_output.size(0), 1).unsqueeze(1)  # [B*1*H]\n",
    "            energy = torch.bmm(v, energy)  # [B*1*seq_len]\n",
    "            return energy.squeeze(1)  # [B*seq_len]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, output_size, method, n_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.method = method\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embed = nn.Embedding(output_size, embed_size)\n",
    "        self.attention = Attention(hidden_size, method)\n",
    "        self.gru = nn.GRU(hidden_size + embed_size, hidden_size,n_layers)\n",
    "        self.linear = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.linear_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, decoder_input, encoder_hidden, encoder_outputs):\n",
    "        \n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attention(encoder_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # (B,1,N)\n",
    "        context = context.transpose(0, 1)  # (1,B,N)\n",
    "        \n",
    "        # Get the embedding of the current input word (previous timestep word output from the decoder)\n",
    "        embedded = self.embed(decoder_input).unsqueeze(0)  # (1,B,N)        \n",
    "        \n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        decoder_input = torch.cat([embedded, context], 2)\n",
    "        output, hidden = self.gru(decoder_input, encoder_hidden)\n",
    "        output = output.squeeze(0)  # (1,B,N) -> (B,N)\n",
    "        context = context.squeeze(0)\n",
    "        attn_hidden_state = F.tanh(self.linear(torch.cat([output, context], 1)))        \n",
    "        output = F.log_softmax(self.linear_out(attn_hidden_state), dim=1)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder/Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(1)\n",
    "        max_len = trg.size(0)\n",
    "        vocab_size = self.decoder.output_size\n",
    "        outputs = torch.zeros(max_len, batch_size, vocab_size).cuda()\n",
    "\n",
    "        encoder_output, hidden = self.encoder(src)\n",
    "        hidden = hidden[:self.decoder.n_layers]\n",
    "        output = trg.data[0, :]  # sos\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, attn_weights = self.decoder(\n",
    "                    output, hidden, encoder_output)\n",
    "            outputs[t] = output\n",
    "            is_teacher = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.data.max(1)[1]\n",
    "            output = trg.data[t] if is_teacher else top1.cuda()\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_iter, model, optimizer, vocab_size, grad_clip):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    pad = EN_TEXT.vocab.stoi['<pad>']\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        src = batch.English\n",
    "        trg = batch.French\n",
    "        #src = torch.FloatTensor(src)\n",
    "        #trg = torch.FloatTensor(trg)\n",
    "        src, trg = src.cuda(), trg.cuda()        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(src, trg)\n",
    "        loss = F.nll_loss(out[1:].view(-1, vocab_size),\n",
    "                               trg[1:].contiguous().view(-1),\n",
    "                               ignore_index=pad)\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        ntokens = (trg[:, 1:] != pad).sum().cpu().item()\n",
    "        total_tokens += ntokens                \n",
    "        \n",
    "        tokens += ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "embed_size = 256\n",
    "lr = 0.0001\n",
    "grad_clip = 10\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "src_vocab_size = len(EN_TEXT.vocab)\n",
    "trg_vocab_size = len(FR_TEXT.vocab)\n",
    "\n",
    "\n",
    "encoder = Encoder(src_vocab_size, embed_size, hidden_size, n_layers=2, dropout=0.2)\n",
    "decoder = Decoder(embed_size, hidden_size, trg_vocab_size, \"concat\", n_layers=1)\n",
    "model = Seq2Seq(encoder, decoder).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 1 Loss: 0.333090 Tokens per Sec: 252.163103\n",
      "Epoch Step: 51 Loss: 0.273615 Tokens per Sec: 289.334309\n",
      "Epoch Step: 101 Loss: 0.239273 Tokens per Sec: 280.089986\n",
      "Epoch Step: 151 Loss: 0.178753 Tokens per Sec: 272.433656\n",
      "Epoch Step: 201 Loss: 0.244191 Tokens per Sec: 280.130982\n",
      "Epoch Step: 251 Loss: 0.234778 Tokens per Sec: 275.392779\n",
      "Epoch Step: 301 Loss: 0.135119 Tokens per Sec: 284.841811\n",
      "Epoch Step: 351 Loss: 0.129239 Tokens per Sec: 288.012579\n",
      "Epoch Step: 401 Loss: 0.074798 Tokens per Sec: 282.449520\n",
      "Epoch Step: 451 Loss: 0.195032 Tokens per Sec: 273.567843\n",
      "Epoch Step: 501 Loss: 0.093449 Tokens per Sec: 291.157303\n",
      "Epoch Step: 551 Loss: 0.186416 Tokens per Sec: 282.007829\n",
      "Epoch Step: 601 Loss: 0.077658 Tokens per Sec: 280.029900\n",
      "Epoch Step: 651 Loss: 0.242201 Tokens per Sec: 282.394647\n",
      "Epoch Step: 701 Loss: 0.240077 Tokens per Sec: 288.684005\n",
      "Epoch Step: 751 Loss: 0.187683 Tokens per Sec: 290.066902\n",
      "Epoch Step: 801 Loss: 0.198470 Tokens per Sec: 294.163255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hveiga/Desktop/Data_Science/dl-venv/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "backward_input can only be called in training mode",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-361ade4e79ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-470b483a85e9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data_iter, model, optimizer, vocab_size, grad_clip)\u001b[0m\n\u001b[1;32m     17\u001b[0m                                \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                ignore_index=pad)\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Data_Science/dl-venv/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Data_Science/dl-venv/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: backward_input can only be called in training mode"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train(train_iter, model, optimizer, trg_vocab_size, grad_clip)\n",
    "    \n",
    "    model.eval()\n",
    "    loss = train(val_iter, model, optimizer, trg_vocab_size, grad_clip)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
